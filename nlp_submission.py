# -*- coding: utf-8 -*-
"""NLP Submission.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cyrEFbBP3AhB1lV2hwBsjRwS-Ky8HJjh
"""

import shutil
import os
from google.colab import files

print("Please put your kaggle.json here... (filename must be kaggle.json)\n");
files.upload()

kaggle_json_file = "/content/kaggle.json"
destination_directory = "/root/.kaggle"

try:
  os.mkdir(destination_directory)
except FileExistsError as e:
  print("Kaggle Directory: ", e)

shutil.copyfile(kaggle_json_file, destination_directory + "/kaggle.json")
! chmod 600 /root/.kaggle/kaggle.json
os.remove(kaggle_json_file)
print("kaggle.json is ready to be served!");

! pip install -q kaggle
! kaggle datasets download -d praveengovi/emotions-dataset-for-nlp

import zipfile;

zip_path = "/content/emotions-dataset-for-nlp.zip"
dataset_zip =  zipfile.ZipFile(zip_path, "r")

dataset = "train.txt"
dataset_save_path = "/tmp/datasets/"
dataset_zip.extract(dataset, dataset_save_path)
new_dataset_name = "dataset.txt"
os.rename(dataset_save_path + dataset, dataset_save_path + new_dataset_name)
print(new_dataset_name, " has been successfully extracted to tmp folder!");

import pandas as pd
dataset = "/tmp/datasets/dataset.txt"
datasets_from_csv = pd.read_csv(dataset, names=['sentence', 'label'], sep=';')
category = pd.get_dummies(datasets_from_csv.label)
datasets_from_csv = datasets_from_csv.drop(columns="label")
datasets_from_csv = pd.concat([datasets_from_csv, category], axis=1)
datasets_from_csv

emotion_labels = ["anger", "fear", "joy", "love", "sadness", "surprise"]
sentence = datasets_from_csv["sentence"].values
labels = datasets_from_csv[emotion_labels].values

from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

sentence_train, sentence_test, labels_train, labels_test = train_test_split(sentence, labels, test_size=0.2)
tokenizer = Tokenizer(num_words=40000, oov_token="-")

tokenizer.fit_on_texts(sentence_train)
tokenizer.fit_on_texts(sentence_test)

train_sequence = tokenizer.texts_to_sequences(sentence_train)
test_sequence = tokenizer.texts_to_sequences(sentence_test)

padded_train = pad_sequences(train_sequence)
padded_test = pad_sequences(test_sequence)

import tensorflow.keras.layers as layers
import tensorflow.keras as kr

class FitCallback(kr.callbacks.Callback):
  def on_train_begin(self, logs=None):
    print("Training has been started!")
  def on_train_end(self, logs=None):
    print("Training has ended!")
  def on_epoch_end(self, batch, logs=None):
    qualify_value = 0.90001
    if (logs["accuracy"] >= qualify_value and logs["val_accuracy"] >= qualify_value):
      print("Accuracy and val_accuracy has reached over 90%. Exit training process...")
      self.model.stop_training = True


model = kr.models.Sequential([layers.Embedding(input_dim=40000, output_dim=64),
                             layers.LSTM(128),
                             layers.Dense(128, activation="relu"),
                             layers.Dense(128, activation="relu"),
                             layers.Dropout(0.4),
                             layers.Dense(64, activation="relu"),
                             layers.Dense(64, activation="relu"),
                             layers.Dense(32, activation="relu"),
                             layers.Dense(6, activation="softmax")])

model.compile(loss="categorical_crossentropy",
              optimizer="Adam",
              metrics=["accuracy"])

fit_callback = FitCallback()
fit_callback.model = model

num_epochs = 60
history = model.fit(padded_train,
                    labels_train,
                    batch_size=50,
                    steps_per_epoch=50,
                    callbacks=[fit_callback],
                    validation_data=(padded_test, labels_test),
                    epochs=num_epochs,
                    verbose=2)

import matplotlib.pyplot as plt

def plot_data(history_keyword):
  plt.plot(history.history[history_keyword])
  plt.title(history_keyword)
  plt.ylabel(history_keyword)
  plt.xlabel("Epochs")
  plt.legend(["Train"], loc="upper right")
  plt.show()

keywords = ["loss", "val_loss", "accuracy", "val_accuracy"]
for keyword in keywords:
  plot_data(keyword)